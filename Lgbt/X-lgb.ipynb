{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import glob\r\n",
    "from joblib import Parallel, delayed\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import scipy as sc\r\n",
    "from sklearn.model_selection import KFold,GroupKFold\r\n",
    "import lightgbm as lgb\r\n",
    "import warnings\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "pd.set_option('max_columns', 300)\r\n",
    "# data directory\r\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'"
   ],
   "outputs": [],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-28T00:53:40.198822Z",
     "iopub.status.busy": "2021-07-28T00:53:40.198018Z",
     "iopub.status.idle": "2021-07-28T00:53:42.400184Z",
     "shell.execute_reply": "2021-07-28T00:53:42.399354Z",
     "shell.execute_reply.started": "2021-07-27T01:13:40.906845Z"
    },
    "papermill": {
     "duration": 2.216519,
     "end_time": "2021-07-28T00:53:42.400394",
     "exception": false,
     "start_time": "2021-07-28T00:53:40.183875",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SEED = 200\r\n",
    "TRIALS =20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "kind = \"k\"\r\n",
    "n_fold = 5\r\n",
    "fileName = f\"{kind}{n_fold}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ParamsDict = {\r\n",
    "    'boosting': 'gbdt',\r\n",
    "    \"metric\": \"rmse\",\r\n",
    "    \"verbosity\": -1,\r\n",
    "    \"max_depth\": -1,\r\n",
    "    'n_estimators': 1200,\r\n",
    "    'seed':SEED,\r\n",
    "    'feature_fraction_seed': SEED,\r\n",
    "    'bagging_seed': SEED,\r\n",
    "    'drop_seed': SEED,\r\n",
    "    'data_random_seed': SEED}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "with open(f\"../input/foldtune/{fileName}X_{SEED}.json\",\"r\") as f:\r\n",
    "    ParamsDict.update(json.load(f))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to calculate first WAP\r\n",
    "def calc_wap1(df):\r\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\r\n",
    "    return wap\r\n",
    "\r\n",
    "# Function to calculate second WAP\r\n",
    "def calc_wap2(df):\r\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\r\n",
    "    return wap\r\n",
    "\r\n",
    "# Function to calculate the log of the return\r\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\r\n",
    "def log_return(series):\r\n",
    "    return np.log(series).diff()\r\n",
    "\r\n",
    "# Calculate the realized volatility\r\n",
    "def realized_volatility(series):\r\n",
    "    return np.sqrt(np.sum(series**2))\r\n",
    "\r\n",
    "# Function to count unique elements of a series\r\n",
    "def count_unique(series):\r\n",
    "    return len(np.unique(series))\r\n",
    "# Function to read our base train and test set\r\n",
    "def read_train_test():\r\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\r\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\r\n",
    "    # Create a key to merge with book and trade data\r\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\r\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\r\n",
    "    print(f'Our training set has {train.shape[0]} rows')\r\n",
    "    return train, test\r\n",
    "# Function to preprocess book data (for each stock id)\r\n",
    "def book_preprocessor(file_path):\r\n",
    "    df = pd.read_parquet(file_path)\r\n",
    "    # Calculate Wap\r\n",
    "    df['wap1'] = calc_wap1(df)\r\n",
    "    df['wap2'] = calc_wap2(df)\r\n",
    "    # Calculate log returns\r\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\r\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\r\n",
    "    \r\n",
    "    df['wap_mean'] = (df['wap1'] + df['wap2']) / 2\r\n",
    "    # Calculate wap balance\r\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\r\n",
    "    # Calculate spread\r\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\r\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\r\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\r\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\r\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\r\n",
    "    \r\n",
    "    df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)/ df[['bid_price1', 'bid_price2']].max(axis = 1) - 1) \r\n",
    "    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\r\n",
    "    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\r\n",
    "    \r\n",
    "    \r\n",
    "    df['log_return_bid_price1'] = np.log(df['bid_price1'].pct_change() + 1)\r\n",
    "    df['log_return_ask_price1'] = np.log(df['ask_price1'].pct_change() + 1)\r\n",
    "    df['log_return_bid_size1'] = np.log(df['bid_size1'].pct_change() + 1)\r\n",
    "    df['log_return_ask_size1'] = np.log(df['ask_size1'].pct_change() + 1)\r\n",
    "    df['log_ask_1_div_bid_1'] = np.log(df['ask_price1'] / df['bid_price1'])\r\n",
    "    df['log_ask_1_div_bid_1_size'] = np.log(df['ask_size1'] / df['bid_size1'])\r\n",
    "    df['log_return_bid_price2'] = np.log(df['bid_price2'].pct_change() + 1)\r\n",
    "    df['log_return_ask_price2'] = np.log(df['ask_price2'].pct_change() + 1)\r\n",
    "    df['log_return_bid_size2'] = np.log(df['bid_size2'].pct_change() + 1)\r\n",
    "    df['log_return_ask_size2'] = np.log(df['ask_size2'].pct_change() + 1)\r\n",
    "    df['log_ask_2_div_bid_2'] = np.log(df['ask_price2'] / df['bid_price2'])\r\n",
    "    df['log_ask_2_div_bid_2_size'] = np.log(df['ask_size2'] / df['bid_size2'])\r\n",
    "    \r\n",
    "    \r\n",
    "    # Dict for aggregations\r\n",
    "    create_feature_dict = {\r\n",
    "        # 测试1\r\n",
    "        'wap1': [np.mean,np.max],#np.sum, 上次测试的结果\r\n",
    "        'wap2': [np.mean,np.max],#np.sum, 上次测试的结果\r\n",
    "        # 如果np.mean, np.max 加入 sum\r\n",
    "        #测试2\r\n",
    "        'log_return1': [realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return2': [realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_bid_price1':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_ask_price1':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_bid_size1':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_ask_size1':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_ask_1_div_bid_1':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_ask_1_div_bid_1_size':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_bid_price2':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_ask_price2':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_bid_size2':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_return_ask_size2':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_ask_2_div_bid_2':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        'log_ask_2_div_bid_2_size':[realized_volatility], # np.sum, np.mean,np.std\r\n",
    "        #测试3\r\n",
    "        'wap_balance': [np.mean, np.max], #上次测试的结果, np.sum\r\n",
    "        'wap_mean':  [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'price_spread': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'bid_spread': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'ask_spread': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'total_volume': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'volume_imbalance': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'bas': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'h_spread_l1': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "        'h_spread_l2': [np.mean, np.max],#上次测试的结果, np.sum\r\n",
    "                       }\r\n",
    "    \r\n",
    "    # Function to get group stats for different windows (seconds in bucket)\r\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\r\n",
    "        # Group by the window\r\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\r\n",
    "        # Rename columns joining suffix\r\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\r\n",
    "        # Add a suffix to differentiate windows\r\n",
    "        if add_suffix:\r\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\r\n",
    "        return df_feature\r\n",
    "    \r\n",
    "    # Get the stats for different windows\r\n",
    "    #根据上周测试还可以加入其他时间段\r\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\r\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\r\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\r\n",
    "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\r\n",
    "    \r\n",
    "    # Merge all\r\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\r\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\r\n",
    "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\r\n",
    "    # Drop unnecesary time_ids\r\n",
    "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\r\n",
    "    \r\n",
    "    # Create row_id so we can merge\r\n",
    "    stock_id = file_path.split('=')[1]\r\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\r\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\r\n",
    "    return df_feature\r\n",
    "# Function to preprocess trade data (for each stock id)\r\n",
    "def trade_preprocessor(file_path):\r\n",
    "    df = pd.read_parquet(file_path)\r\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\r\n",
    "    \r\n",
    "    # Dict for aggregations\r\n",
    "    #根据上周测试调整\r\n",
    "    create_feature_dict = {\r\n",
    "        'log_return':[realized_volatility],\r\n",
    "        'seconds_in_bucket':[count_unique],\r\n",
    "        'size':[np.sum],\r\n",
    "        'order_count':[np.mean],\r\n",
    "    }\r\n",
    "    \r\n",
    "    # Function to get group stats for different windows (seconds in bucket)\r\n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\r\n",
    "        # Group by the window\r\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\r\n",
    "        # Rename columns joining suffix\r\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\r\n",
    "        # Add a suffix to differentiate windows\r\n",
    "        if add_suffix:\r\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\r\n",
    "        return df_feature\r\n",
    "    \r\n",
    "    # Get the stats for different windows\r\n",
    "    #根据上周测试还可以加入其他时间段\r\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\r\n",
    "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\r\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\r\n",
    "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\r\n",
    "\r\n",
    "    # Merge all\r\n",
    "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\r\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\r\n",
    "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\r\n",
    "    # Drop unnecesary time_ids\r\n",
    "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\r\n",
    "    \r\n",
    "    df_feature = df_feature.add_prefix('trade_')\r\n",
    "    stock_id = file_path.split('=')[1]\r\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\r\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\r\n",
    "    return df_feature"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to get group stats for the stock_id and time_id\r\n",
    "def get_time_stock(df):\r\n",
    "    # Get realized volatility columns\r\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \r\n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \r\n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\r\n",
    "\r\n",
    "    # Group by the stock id\r\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\r\n",
    "    # Rename columns joining suffix\r\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\r\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\r\n",
    "\r\n",
    "    # Group by the time id\r\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\r\n",
    "    # Rename columns joining suffix\r\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\r\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\r\n",
    "    \r\n",
    "    # Merge with original dataframe\r\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\r\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\r\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\r\n",
    "def preprocessor(list_stock_ids, is_train = True):\r\n",
    "    \r\n",
    "    # Parrallel for loop\r\n",
    "    def for_joblib(stock_id):\r\n",
    "        # Train\r\n",
    "        if is_train:\r\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\r\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\r\n",
    "        # Test\r\n",
    "        else:\r\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\r\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\r\n",
    "    \r\n",
    "        # Preprocess book and trade data and merge them\r\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\r\n",
    "        \r\n",
    "        # Return the merge dataframe\r\n",
    "        return df_tmp\r\n",
    "    \r\n",
    "    # Use parallel api to call paralle for loop\r\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\r\n",
    "    # Concatenate all the dataframes that return from Parallel\r\n",
    "    df = pd.concat(df, ignore_index = True)\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to calculate the root mean squared percentage error\r\n",
    "def rmspe(y_true, y_pred):\r\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\r\n",
    "\r\n",
    "# Function to early stop with root mean squared percentage error\r\n",
    "def feval_rmspe(y_pred, lgb_train):\r\n",
    "    y_true = lgb_train.get_label()\r\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# feature importance\r\n",
    "DO_FEAT_IMP =  True\r\n",
    "def calc_model_importance(model, feature_names=None, importance_type='gain'):\r\n",
    "    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\r\n",
    "                                 index=feature_names,\r\n",
    "                                 columns=['importance']).sort_values('importance')\r\n",
    "    return importance_df\r\n",
    "\r\n",
    "\r\n",
    "def plot_importance(importance_df, title='',\r\n",
    "                    save_filepath=None, figsize=(15, 20)):\r\n",
    "    fig, ax = plt.subplots(figsize=figsize)\r\n",
    "    importance_df.plot.barh(ax=ax)\r\n",
    "    if title:\r\n",
    "        plt.title(title)\r\n",
    "    plt.tight_layout()\r\n",
    "    if save_filepath is None:\r\n",
    "        plt.show()\r\n",
    "    else:\r\n",
    "        plt.savefig(save_filepath)\r\n",
    "    plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_and_evaluate(train, test,DO_FEAT_IMP = True):\r\n",
    "    # Hyperparammeters (just basic)\r\n",
    "    params = ParamsDict\r\n",
    "    \r\n",
    "    # Split features and target\r\n",
    "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\r\n",
    "    y = train['target']\r\n",
    "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\r\n",
    "    # Transform stock id to a numeric value\r\n",
    "    x['stock_id'] = x['stock_id'].astype(int)\r\n",
    "    x_test['stock_id'] = x_test['stock_id'].astype(int)\r\n",
    "    \r\n",
    "    # Create out of folds array\r\n",
    "    oof_predictions = np.zeros(x.shape[0])\r\n",
    "    # Create test array to store predictions\r\n",
    "    test_predictions = np.zeros(x_test.shape[0])\r\n",
    "    # Create a KFold object\r\n",
    "    if kind == \"k\":\r\n",
    "        kfold = KFold(n_splits = n_fold, random_state = SEED, shuffle = True) #上周一样\r\n",
    "    elif (kind == \"g\"):\r\n",
    "        kfold = GroupKFold(n_splits = n_fold, random_state = SEED, shuffle = True)\r\n",
    "    # Iterate through each fold\r\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\r\n",
    "        print(f'Training fold {fold + 1}')\r\n",
    "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\r\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\r\n",
    "        # Root mean squared percentage error weights\r\n",
    "        train_weights = 1 / np.square(y_train)\r\n",
    "        val_weights = 1 / np.square(y_val)\r\n",
    "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\r\n",
    "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\r\n",
    "        model = lgb.train(params = params, \r\n",
    "                          train_set = train_dataset, \r\n",
    "                          valid_sets = [train_dataset, val_dataset], \r\n",
    "                          num_boost_round = 10000, \r\n",
    "                          early_stopping_rounds = 50, \r\n",
    "                          verbose_eval = 50,\r\n",
    "                          feval = feval_rmspe)\r\n",
    "        # Add predictions to the out of folds array\r\n",
    "        oof_predictions[val_ind] = model.predict(x_val)\r\n",
    "        # Predict the test set\r\n",
    "        test_predictions += model.predict(x_test) / 5\r\n",
    "        \r\n",
    "    rmspe_score = rmspe(y, oof_predictions)\r\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\r\n",
    "    gain_importance_list = []\r\n",
    "    split_importance_list = []\r\n",
    "    if DO_FEAT_IMP:    \r\n",
    "        feature_names = x_train.columns.values.tolist()\r\n",
    "        gain_importance_df = calc_model_importance(\r\n",
    "            model, feature_names=feature_names, importance_type='gain')\r\n",
    "        gain_importance_list.append(gain_importance_df)\r\n",
    "\r\n",
    "        split_importance_df = calc_model_importance(\r\n",
    "            model, feature_names=feature_names, importance_type='split')\r\n",
    "        split_importance_list.append(split_importance_df)\r\n",
    "    # Return test predictions\r\n",
    "    return test_predictions,gain_importance_list,split_importance_list\r\n",
    "\r\n",
    "# Read train and test\r\n",
    "train, test = read_train_test()\r\n",
    "\r\n",
    "# Get unique stock ids \r\n",
    "train_stock_ids = train['stock_id'].unique()\r\n",
    "# Preprocess them using Parallel and our single stock id functions\r\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\r\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\r\n",
    "\r\n",
    "# Get unique stock ids \r\n",
    "test_stock_ids = test['stock_id'].unique()\r\n",
    "# Preprocess them using Parallel and our single stock id functions\r\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\r\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\r\n",
    "\r\n",
    "# Get group stats of time_id and stock_id\r\n",
    "train = get_time_stock(train)\r\n",
    "test = get_time_stock(test)\r\n",
    "\r\n",
    "# Traing and evaluate\r\n",
    "test_predictions,gain_importance_list,split_importance_list = train_and_evaluate(train, test,DO_FEAT_IMP = True)\r\n",
    "\r\n",
    "# Save test predictions\r\n",
    "test['target'] = test_predictions\r\n",
    "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-28T00:53:42.558512Z",
     "iopub.status.busy": "2021-07-28T00:53:42.418442Z",
     "iopub.status.idle": "2021-07-28T02:09:48.110482Z",
     "shell.execute_reply": "2021-07-28T02:09:48.111352Z",
     "shell.execute_reply.started": "2021-07-27T01:13:59.221125Z"
    },
    "papermill": {
     "duration": 4565.706466,
     "end_time": "2021-07-28T02:09:48.111652",
     "exception": false,
     "start_time": "2021-07-28T00:53:42.405186",
     "status": "completed"
    },
    "tags": []
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4577.144316,
   "end_time": "2021-07-28T02:09:49.447289",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-28T00:53:32.302973",
   "version": "2.3.3"
  },
  "interpreter": {
   "hash": "4876ff34b70794a54711585a56035755ce9dd6f9a98e80662ac35fb37c287d01"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}